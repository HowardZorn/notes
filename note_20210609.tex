% !TEX TS-program = xelatex
% !TEX encoding = UTF-8

\documentclass{ctexart}
\usepackage{note}

\title{Multi-Task Learning笔记}
\author{Fw[a]rd\thanks{这篇文章的内容以GPLv2协议释出}}
\date{2021年6月9日}

\begin{document}
\maketitle

\section{因缘}
因为某个项目的原因，需要考虑多个loss的平衡问题。在\href{https://www.zhihu.com/question/375794498}{知乎}用户的推荐下阅读了：

\begin{itemize}
    \item Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\cite{Kendall18Uncertainty}
    \item GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\cite{Chen18GradNorm}
    \item Multi-Task Learning as Multi-Objective Optimization\cite{Sener18Pareto}
\end{itemize}

这几篇文章各有千秋，方法论不尽相同。

\section{内容}
\subsection{背景}
\subsubsection{Multi-Task Learning}
即把多个相关的任务放在一起学习。多个任务之间共享一些参数，它们可以在学习过程中共享它们所学到的信息，这是单任务学习所不具备的。相关联的多任务学习比单任务学习能去的更好的泛化效果。更加广义地，只要有多个loss就算multi-task learning。

\subsection{方法}

之前多重任务学习问题大多采用多个loss的加权和作为整个任务的loss：
\begin{equation}
    L_\mathrm{total} = \sum_i w_iL_i.
\end{equation}
这些权值$w$通常是不变的并且需要手动调整，这就让获得最优权值的过程很麻烦。

\subsubsection{\citefullauthor{Kendall18Uncertainty}}

\citet{Kendall18Uncertainty}提出，多重任务学习中每个任务最优的权重由其测量尺度——更详细地，由其噪声的大小决定。他们将\textbf{同方差不确定性}理解为与任务相关的权值，基于同方差不确定性的高斯极大似然推导出了一套多重任务loss。

假设$f^W(x)$是神经网络$f^W(\cdot)$的输出，$W$是神经网络的权值，$x$是输入。那么对于回归问题和分类问题的概率模型可建模为：
\begin{equation}
    \label{eq:output-likelihood}
    \begin{aligned}
        p(y|f^W(x)) &= \mathcal{N}(f^W(x), \sigma^2) = {\frac {1}{\sigma {\sqrt {2\pi }}}}\;e^{-{\frac {\left(y-f^W(x) \right)^{2}}{2\sigma ^{2}}}}, &\textrm{a.回归问题},\\
        p(y|f^W(x)) &= \mathrm{Softmax}(f^W(x)) \xlongequal{\textrm{第i个元素}} \frac{e^{f^W_i(x)}}{\sum_{i} e^{f^W_i(x)}}, &\textrm{b.分类问题},
    \end{aligned}
\end{equation}
其中，$\mathcal{N}$是高斯分布，$\sigma$是代表观测噪声水平的标量。

对式~\ref{eq:output-likelihood}.a~做对数，可得其对数似然正比于：
\begin{equation}
    \label{eq:回归问题公式}
    \begin{aligned}
        \log p(y|f^W(x)) &\propto - \frac{1}{2\sigma^2}\|y-f^W(x)\|^2 - \log \sigma \\
        &= -\frac{1}{2\sigma^2}\mathcal{L_\mathrm{MSE}}(W) - \log \sigma.
    \end{aligned}
\end{equation}

对式~\ref{eq:output-likelihood}.b~进行缩放，然后对$p(y|f^W(x)) = \mathrm{Softmax}(\frac{1}{\sigma^2} f^W(x))$\footnote{可以认为是\href{https://zh.wikipedia.org/wiki/玻尔兹曼分布}{玻尔兹曼分布}}做对数，可得：
\begin{equation}
    \log p(y|f^W(x)) \xlongequal{\textrm{第i个元素}} \frac{1}{\sigma^2} f^W_i(x) - \log \sum_e \exp\left( \frac{1}{\sigma^2} f^W_e(x)\right).
\end{equation}

然后可推对于分类问题不进行缩放的对数似然函数（不要问我为什么，反复看了论文数次没有看懂……）：% TODO 搞懂公式推导过程……
\begin{equation}
    \label{eq:分类问题公式}
    \log p(y|f^W(x)) \approx -\frac{1}{\sigma^2}\mathcal{L_\mathrm{Cross Entropy}}(W) - \log \sigma.
\end{equation}

对于\emph{多}个任务的多个输出，其极大似然及对数似然可以表示为：
\begin{equation}
    \begin{aligned}
        p(y_1, \ldots, y_k|f^W(x)) &= p(y_1 | f^W(x)) \cdots p(y_k | f^W(x)), \\
        &\downarrow \\
        \log p(y_1, \ldots, y_k|f^W(x)) &= \log p(y_1 | f^W(x)) + \cdots + \log p(y_k | f^W(x)).
    \end{aligned}
\end{equation}

因为loss追求最小化，所以给多个任务的对数似然添加负号之后就是我们要的多重任务loss函数$\mathcal{L}(\cdot)$，可以使用式~\ref{eq:回归问题公式}~和式~\ref{eq:分类问题公式}的结果对其中的单个任务的输出的对数似然进行替换。
\begin{equation}
    \mathcal{L}(W) = - \log p(y_1, \ldots, y_k|f^W(x)) = - (\log p(y_1 | f^W(x)) + \cdots + \log p(y_k | f^W(x))).
\end{equation}
在这个多重任务loss中，代表噪声的$\sigma$将作为可学习的参数。因为在$\log(\cdot)$下$\sigma$可能取到非法的值，所以我们重新定义一个可学习参数$s=\log \sigma^2$代替式子中的$\sigma$。

\paragraph{例} 假设有一个多重任务，其中一个是回归问题（$\mathcal{L}_1$是MSE），一个是分类问题（$\mathcal{L}_2$是交叉熵）。那么它总的loss是：
\begin{equation}
    \begin{aligned}
        \mathcal{L}(W) &= - \log p(y_1|f^W(x)) - \log p(y_2|f^W(x)) \\
         &= \frac{1}{2{\sigma_1}^2} \mathcal{L}_1(W) + \frac{1}{{\sigma_2}^2} \mathcal{L}_2(W) + \log \sigma_1\sigma_2 \\
         &\xlongequal{s_i =\log {\sigma_i}^2} \frac{1}{2{e}^{s_1}} \mathcal{L}_1(W) + \frac{1}{{e}^{s_2}} \mathcal{L}_2(W) + \frac{s_1 + s_2}{2}.
    \end{aligned}
\end{equation}

\subsubsection{\citefullauthor{Chen18GradNorm}}

\citet{Chen18GradNorm}提出，任务间的不平衡的表现为反向传播时\emph{梯度的不平衡}，最终影响训练结果。比方说有一个占主导地位的任务，其主导地位表现为反向传播时具有相对其它任务而言较大的梯度。他们通过调整每个任务的损失函数的反向传播梯度的大小，使多个任务在相似的\emph{速率}下训练。

这篇文章的目标是找出在某一步训练$t$时，对应某个任务$i$的最佳权值$w_i(t)$，即最终的多重任务loss $L_\mathrm{total}(\cdot)$为：
\begin{equation}
    \label{eq:GradNormTotalLoss}
    L_\mathrm{total}(W;t) = \sum_i w_i(t) L_i.
\end{equation}

为了学习$w_i(t)$，需要将不同任务的梯度范数置于同一个量度下，获得其相对的大小；并动态调整这些梯度范数的大小，以便不同的任务以相似的速率训练。下面对一些需要使用的量进行定义：
\begin{itemize}
    \item $W$：神经网络中应用GradNorm方法的那部分网络的权值。为了节约计算成本，通常是多重任务网络中的最后一层共享层的权值。
    \item $G_{W}^{(i)}(t) =\left \| \bigtriangledown_{W}w_{i}(t)L_{i}(t) \right \|_{2}$：选择的权值$W$对应的加权单任务loss\ $w_{i}(t)L_{i}(t)$的梯度的L2范数。
    \item $\bar{G}_{W}(t)=E_\mathrm{task}[G_{W}^{(i)}(t)]$：在$t$时刻，所有任务的平均梯度范数。
    \item $\tilde{L}_{i}(t)={L_{i}(t)}/{L_{i}(0)}$：任务$i$在时刻$t$与初始时刻的loss比例，$\tilde{L}_{i}(t)$反比于训练速率，越小则训练越快。 % TODO L(0) 的设置和选择
    \item $r_{i}(t)=\tilde{L}_{i}(t)/E_\mathrm{task}[\tilde{L}_{i}(t)]$：任务$i$的相对反比训练速率。
\end{itemize}

$\bar{G}_{W}(t)$用来衡量在$t$时刻平均梯度范数的大小并由此确定相对梯度大小。$r_{i}(t)$则用来平衡梯度，如果$r_{i}(t)$越大，则任务$i$的梯度大小应该越高以加快任务的训练。因此，任务$i$的梯度范数$G_{W}^{(i)}(t)$的目标是：
\begin{equation}
    G_{W}^{(i)}(t) \mapsto \bar{G}_W(t) \times [r_i(t)]^\alpha, 
\end{equation}
其中，$\alpha$是设定恢复力强度的超参数，即将任务的训练速度调节到平均水准的强度。如果任务的复杂程度很不一样，导致任务之间的学习速率大不相同，就应该使用较高的$\alpha$来进行较强的训练速率平衡；反之，对于多个相似的任务，应该使用较小的$\alpha$。

既然有了目标梯度范数，那么我们针对式~\ref{eq:GradNormTotalLoss}~中$w_i(t)$的loss函数如下：
\begin{equation}
    L_{grad}(t;w_{i}(t))=\sum_{i}\left \| G_{w}^{(i)}(t)-\bar{G}_{w}(t) \times [r_{i}(t)]^{\alpha} \right \|_{1},
\end{equation}
其中，在微分$L_{grad}$时，仅仅针对$w_i$，并且$\bar{G}_{w}(t)\times[r_{i}(t)]^{\alpha}$被看作为常数。另外，在每次更新$w_i(t)$前，都会重新标准化$w_i(t)$使得$\sum_{i}w_{i}(t)=T$。这使得GradNorm方法对$w_i$的调整与全局学习率的设置解耦\footnote{如果$\sum_{i}w_{i}(t)\neq T$，那么全局学习率的设置就仅仅只有指导意义了。想象一下$\sum_{i}w_{i}(t)$过大和过小对全局学习率的影响。实际上，手动调节每个任务的权值时也需要满足这个要求。}。

完整算法如算法~\ref{alg:gradnorm}~所示。

\begin{algorithm}[htb]
    \caption{使用GradNorm的训练过程}
    \label{alg:gradnorm}
 \begin{algorithmic}
    \STATE 初始化 $w_i(0)=1$ $\forall i$
    \STATE 初始化网络权值 $\mathcal{W}$
    \STATE 选择一个$\alpha> 0$，然后选择需要进行GradNorm的网络权值$W$（通常是多个任务间共享的\\ \hspace{1em} 最后一层）
    \FOR{$t=0 \to max\_train\_steps$}
    \STATE {\bfseries 输入}一批次$x_i$以计算$L_i(t)$ $\forall i$ 和 $L(t) = \sum_i w_i(t)L_i(t)$ {\fangsong [一般前向流程]}
    \STATE 计算 $G_W^{(i)}(t)$ 和 $r_i(t)$ $\forall i$
    \STATE 计算 $\bar{G}_W(t)$，通过对 $G_W^{(i)}(t)$ 做平均
    \STATE 计算 $L_{\text{grad}}= \sum_i\rvert G_W^{(i)}(t) - \bar{G}_W(t)\times [r_i(t)]^{\alpha}\rvert_1$
    \STATE 计算GradNorm的梯度$\nabla_{w_i} L_{\text{grad}}$，将目标$\bar{G}_W(t)\times [r_i(t)]^{\alpha}$看作常数。
    \STATE 计算一般的梯度$\nabla_{\mathcal{W}} L(t)$
    \STATE 更新$w_i(t) \mapsto w_i(t+1)$，使用梯度$\nabla_{w_i} L_{\text{grad}}$
    \STATE 更新$\mathcal{W}(t) \mapsto \mathcal{W}(t+1)$，使用梯度$\nabla_{\mathcal{W}} L(t)$ {\fangsong [一般反向流程]}
    \STATE 重新标准化$w_i(t+1)$，使得$\sum_iw_i(t+1) = T$
    \ENDFOR
 \end{algorithmic}
 \end{algorithm}

\subsubsection{\citefullauthor{Sener18Pareto}}

\citet{Sener18Pareto}提出，

\subsection{实现}

\subsubsection{\citefullauthor{Kendall18Uncertainty}}

官方实现是一个空的仓库，很有意思……（GitHub: \href{https://github.com/dyz-zju/multitaskvision}{dyz-zju/multitaskvision}，该地址疑为fork过后的）有非官方实现（GitHub: \href{https://github.com/ranandalon/mtl}{ranandalon/mtl}, \href{https://github.com/Mikoto10032/AutomaticWeightedLoss}{Mikoto\-10032/Automatic\-Weighted\-Loss}）。

下面是来自Mikoto\-10032/Automatic\-Weighted\-Loss仓库的核心代码和使用示范：
\begin{minted}[
    frame=lines,
    linenos
]{python}
import torch
import torch.nn as nn

class AutomaticWeightedLoss(nn.Module):
    """automatically weighted multi-task loss
    Params：
        num: int，the number of loss
        x: multi-task loss
    Examples：
        loss1=1
        loss2=2
        awl = AutomaticWeightedLoss(2)
        loss_sum = awl(loss1, loss2)
    """
    def __init__(self, num=2):
        super(AutomaticWeightedLoss, self).__init__()
        params = torch.ones(num, requires_grad=True)
        self.params = torch.nn.Parameter(params)

    def forward(self, *x):
        loss_sum = 0
        for i, loss in enumerate(x):
            loss_sum += 0.5 / (self.params[i] ** 2) * loss + \
                torch.log(1 + self.params[i] ** 2)
        return loss_sum

# Demo
from torch import optim

model = Model()
optimizer = optim.Adam([
                {'params': model.parameters()},
                {'params': awl.parameters(), 'weight_decay': 0}
            ])
\end{minted}

\subsubsection{\citefullauthor{Chen18GradNorm}}

没有找到官方实现。有一些非官方实现（GitHub: \href{https://github.com/brianlan/pytorch-grad-norm}{brianlan/pytorch-grad-norm}（简易版）、\href{https://github.com/brianlan/complex-grad-norm}{brianlan/complex-grad-norm}（复杂版））。

下面是本人仿写的代码：
\begin{minted}[
    frame=lines,
    linenos
]{python}
import torch
import torch.nn as nn
import torch.optim as optim

class GradNormLoss(nn.Module):
    def __init__(self, num_of_task, alpha=1.5):
        super(GradNormLoss, self).__init__()
        self.num_of_task = num_of_task
        self.alpha = alpha
        self.w = nn.Parameter(torch.ones(num_of_task, dtype=torch.float))
        self.l1_loss = nn.L1Loss()
        self.L_0 = None

    # standard forward pass
    def forward(self, L_t: torch.Tensor):
        # initialize the initial loss `Li_0`
        if self.L_0 is None:
            self.L_0 = L_t.detach() # detach
        # compute the weighted loss w_i(t) * L_i(t)
        self.L_t = L_t
        self.wL_t = L_t * self.w
        # the reduced weighted loss
        self.total_loss = self.wL_t.sum()
        return self.total_loss

    # additional forward & backward pass
    def additional_forward_and_backward(self, grad_norm_weights: nn.Module, 
            optimizer: optim.Optimizer):
        # do `optimizer.zero_grad()` outside
        self.total_loss.backward(retain_graph=True)
        # in standard backward pass, `w` does not require grad
        self.w.grad.data = self.w.grad.data * 0.0

        self.GW_t = []
        for i in range(self.num_of_task):
            # get the gradient of this task loss with respect to the shared parameters
            GiW_t = torch.autograd.grad(
                self.L_t[i], grad_norm_weights.parameters(),
                    retain_graph=True, create_graph=True)
            # compute the norm
            self.GW_t.append(torch.norm(GiW_t[0] * self.w[i]))
        self.GW_t = torch.stack(self.GW_t) # do not detatch
        self.bar_GW_t = self.GW_t.detach().mean()
        self.tilde_L_t = (self.L_t / self.L_0).detach()
        self.r_t = self.tilde_L_t / self.tilde_L_t.mean()
        grad_loss = self.l1_loss(self.GW_t, self.bar_GW_t * (self.r_t ** self.alpha))
        self.w.grad = torch.autograd.grad(grad_loss, self.w)[0]
        optimizer.step()

        self.GW_ti, self.bar_GW_t, self.tilde_L_t, 
            self.r_t, self.L_t, self.wL_t = None, None, None, None, None, None
        # re-norm
        self.w.data = self.w.data / self.w.data.sum() * self.num_of_task

# This is AN interface.
class GradNormModel:
    def get_grad_norm_weights(self) -> nn.Module:
        raise NotImplementedError(
            "Please implement the method `get_grad_norm_weights`")
\end{minted}

\subsubsection{\citefullauthor{Sener18Pareto}}

有官方实现（GitHub: \href{https://github.com/intel-isl/MultiObjectiveOptimization}{intel-isl/MultiObjectiveOptimization}）。

\section{结果与观感}

\subsection{\citefullauthor{Kendall18Uncertainty}}

有很多证据表明这个方法并不那么有效，不过至少提供了个思路。

对于我的任务而言，效果不是很好，只在一个loss取得了不错的效果，其它几个loss有明显的退步。

\subsection{\citefullauthor{Chen18GradNorm}}

\subsection{\citefullauthor{Sener18Pareto}}

\bibliography{ref/note_20210609}

\end{document}