% !TEX TS-program = xelatex
% !TEX encoding = UTF-8

\documentclass[a4paper,punct]{ctexart}
\usepackage{note}

\title{Multi-Task Learning 笔记}
\author{Fw[a]rd\thanks{这篇文章的内容以GPLv2协议释出}}
\date{2021年6月9日}

\begin{document}
\maketitle

\section{因缘}
因为某个项目的原因，需要考虑多个loss的平衡问题。在\href{https://www.zhihu.com/question/375794498}{知乎}用户的推荐下阅读了：

\begin{itemize}
    \item Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\cite{Kendall18Uncertainty}
    \item GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\cite{Chen18GradNorm}
    \item Multi-Task Learning as Multi-Objective Optimization\cite{Sener18Pareto}
\end{itemize}

这几篇文章各有千秋，方法论不尽相同。

\section{内容}
\subsection{背景}
\subsubsection{Multi-Task Learning}
把多个相关的任务放在一起学习，同时学习多个任务。多个任务之间共享一些因素，它们可以在学习过程中，共享它们所学到的信息，这是单任务学习所不具备的。相关联的多任务学习比单任务学习能去的更好的泛化效果。更加广义地，只要有多个loss就算multi-task learning。

\subsection{方法}

之前多重任务学习问题大多采用多个loss的加权和作为整个任务的loss：

\begin{equation}
    L_\mathrm{total} = \sum_i w_iL_i
\end{equation}

这些权值$w$通常是不变的并且需要手动调整，这就让获得最优权值的过程很麻烦。

\subsubsection{\citefullauthor{Kendall18Uncertainty}}

\citet{Kendall18Uncertainty}提出，多重任务学习中每个任务最优的权重由其测量尺度——更详细地，由其噪声的大小决定。他们将\textbf{同方差不确定性}理解为与任务相关的权值，基于同方差不确定性的高斯极大似然推导出了一套多重任务loss。

假设$f^W(x)$是神经网络$f^W(\cdot)$的输出，$W$是神经网络的权值，$x$是输入。那么对于回归问题和分类问题的概率模型可建模为：

\begin{equation}
    \label{eq:output-likelihood}
    \begin{aligned}
        p(y|f^W(x)) &= \mathcal{N}(f^W(x), \sigma^2) = {\frac {1}{\sigma {\sqrt {2\pi }}}}\;e^{-{\frac {\left(y-f^W(x) \right)^{2}}{2\sigma ^{2}}}}, &\textrm{a.回归问题},\\
        p(y|f^W(x)) &= \mathrm{Softmax}(f^W(x)) \xlongequal{\textrm{第i个元素}} \frac{e^{f^W_i(x)}}{\sum_{i} e^{f^W_i(x)}}, &\textrm{b.分类问题}.
    \end{aligned}
\end{equation}

其中，$\mathcal{N}$是高斯分布，$\sigma$是代表观测噪声水平的标量。

对式~\ref{eq:output-likelihood}.a~做对数，可得其对数似然正比于：

\begin{equation}
    \begin{aligned}
        \log p(y|f^W(x)) &\propto - \frac{1}{2\sigma^2}\|y-f^W(x)\|^2 - \log \sigma \\
        &= -\frac{1}{2\sigma^2}\mathcal{L_\mathrm{MSE}}(W) - \log \sigma 
    \end{aligned}
\end{equation}

对式~\ref{eq:output-likelihood}.b~进行缩放，然后对$p(y|f^W(x)) = \mathrm{Softmax}(\frac{1}{\sigma^2} f^W(x))$\footnote{可以认为是\href{https://zh.wikipedia.org/wiki/玻尔兹曼分布}{玻尔兹曼分布}}做对数，可得：

\begin{equation}
    \begin{aligned}
        \log p(y|f^W(x)) &\xlongequal{\textrm{第i个元素}} \frac{1}{\sigma^2} f^W_i(x) - \log \sum_e \exp\left( \frac{1}{\sigma^2} f^W_e(x)\right) \\
        &\approx -\frac{1}{\sigma^2}\mathcal{L_\mathrm{Cross Entropy}}(W) - \log \sigma 
    \end{aligned}
\end{equation}

\subsection{实现}

\subsubsection{\citefullauthor{Kendall18Uncertainty}}

似乎没有官方实现。有非官方实现（GitHub: \href{https://github.com/ranandalon/mtl}{ranandalon/mtl}）。

% \begin{minted}{python}
% class PositionalEncoding(nn.Module):
%     '''
%     Positional Encoding
%     '''
%     def __init__(self):
%         super(PositionalEncoding, self).__init__()

%     def forward(self, shape, device):
%         '''
%         shape: Size(batch_size, P, N, D)
%         device: tensor's device type
%         return: [batch_size, P, N, D]
%         '''
%         batch_size, P, N, D = shape
%         return torch_utils.positional_encoding_traffic(batch_size, P, N, D).to(device)
% \end{minted}

\bibliography{ref/note_20210609}

\end{document}